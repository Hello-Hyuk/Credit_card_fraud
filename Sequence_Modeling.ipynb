{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sequence_Modeling.ipynb","provenance":[],"authorship_tag":"ABX9TyNcweHStEGZdF5gXuRwUz7B"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"IQeFZ2WrtQ85"},"outputs":[],"source":["# 소현용\n","%cd /content/drive/MyDrive/AI_project/6.Credit_card_fraud/Credit_card_fraud\n","\n","! git config --global user.email \"baeksh.0420@gmail.com\"\n","! git config --global user.name \"baeksh0420\""]},{"cell_type":"code","source":["! git checkout feature/baeksh0420"],"metadata":{"id":"_E0PWZE7tpyb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! git add .\n","! git commit -m \"try model\"\n","! git push"],"metadata":{"id":"1dc3rBYjtpwG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"_f6pjCx7tptz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","from sklearn.ensemble import IsolationForest\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import classification_report\n","from sklearn.cluster import *\n","\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np\n","import time\n","\n","import re\n","import io\n","\n","\n","import warnings\n","warnings.filterwarnings(action='ignore')\n","# "],"metadata":{"id":"tR3GzECWtprN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df = pd.read_csv('dataset/train.csv') # Train\n","train_df.head()"],"metadata":{"id":"4fydmXb4tstT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_df = pd.read_csv('dataset/val.csv') # Validation\n","val_df.head()"],"metadata":{"id":"BtH_0sVotsq7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["train"],"metadata":{"id":"Y7wXKTqOt6JI"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","# 인코더(Encoder) 아키텍처 정의\n","class Encoder(nn.Module):\n","    def __init__(self, input_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):\n","        super().__init__()\n","\n","        # Embedding 레이어\n","        self.embedding = nn.Embedding(input_dim, embed_dim)\n","\n","        # LSTM 레이어\n","        self.hid_dim = hidden_dim\n","        self.n_layers = n_layers\n","        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)\n","        \n","        self.dropout = nn.Dropout(dropout_ratio)\n","\n","    # 최종적으로 context vector를 반환(hidden, cell의 모습)\n","    # 임베딩을 거치고, rnn을 거쳐 결과값들이 반환된다.\n","    def forward(self, src):\n","        embedded = self.dropout(self.embedding(src))\n","\n","        outputs, (hidden, cell) = self.rnn(embedded)\n","\n","        # 문맥 벡터(context vector) 반환\n","        return hidden, cell"],"metadata":{"id":"Zk2tCb14tso1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 디코더(Decoder) 아키텍처 정의\n","class Decoder(nn.Module):\n","    def __init__(self, output_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):\n","        super().__init__()\n","\n","        #Embedding 레이어\n","        self.embedding = nn.Embedding(output_dim, embed_dim)\n","\n","        # LSTM 레이어\n","        self.hid_dim = hidden_dim\n","        self.n_layers = n_layers\n","        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)\n","        \n","        # fully_connected fpdldj\n","        self.output_dim = output_dim\n","        self.fc_ = nn.Linear(hidden_dim, output_dim)\n","\n","        self.dropout = nn.Dropout(dropout_ratio)\n","\n","    # 각 과정마다 단어를 출력, 출력된 문장의 값들을 hidden과 cell로 출력   \n","    def forward(self, input, hidden, cell):\n","        input = input.unsqueeze(0)\n","        \n","        embedded = self.dropout(self.embedding(input))\n","\n","        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","\n","        result = self.fc_(output.squeeze(0))\n","        \n","        # result는 현재 출력 단어\n","        return result, hidden, cell"],"metadata":{"id":"FL6-5j7Ttsmv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super().__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","    # 학습할 때는 완전한 형태의 소스 문장, 타겟 문장, teacher_forcing_ratio를 넣기\n","    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","\n","        hidden, cell = self.encoder(src)\n","\n","        # 디코더(decoder)의 최종 결과를 담을 텐서 객체 만들기\n","        trg_len = trg.shape[0] # 단어 개수\n","        batch_size = trg.shape[1] # 배치 크기\n","        trg_vocab_size = self.decoder.output_dim # 출력 차원\n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","\n","        # 첫 번째 입력은 항상 <sos> 토큰\n","        input = trg[0, :]\n","\n","        # 타겟 단어의 개수만큼 반복하여 디코더에 포워딩(forwarding)\n","        for t in range(1, trg_len):\n","            output, hidden, cell = self.decoder(input, hidden, cell)\n","\n","            outputs[t] = output # FC를 거쳐서 나온 현재의 출력 단어 정보\n","            top1 = output.argmax(1) # 가장 확률이 높은 단어의 인덱스 추출\n","\n","            # teacher_forcing_ratio: 학습할 때 실제 목표 출력(ground-truth)을 사용하는 비율\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            input = trg[t] if teacher_force else top1 # 현재의 출력 결과를 다음 입력에서 넣기"],"metadata":{"id":"Ek-NHIjotskb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["INPUT_DIM = len(SRC.vocab)\n","OUTPUT_DIM = len(TRG.vocab)\n","ENCODER_EMBED_DIM = 256\n","DECODER_EMBED_DIM = 256\n","HIDDEN_DIM = 512\n","N_LAYERS = 2\n","ENC_DROPOUT_RATIO = 0.5\n","DEC_DROPOUT_RATIO = 0.5\n","\n","# 인코더(encoder)와 디코더(decoder) 객체 선언\n","enc = Encoder(INPUT_DIM, ENCODER_EMBED_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT_RATIO)\n","dec = Decoder(OUTPUT_DIM, DECODER_EMBED_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT_RATIO)\n","\n","# Seq2Seq 객체 선언\n","model = Seq2Seq(enc, dec, device).to(device)\n","\n","def init_weights(m):\n","    for name, param in m.named_parameters():\n","        nn.init.uniform_(param.data, -0.08, 0.08)\n","        \n","model.apply(init_weights)"],"metadata":{"id":"gi-jSb2Ptpo4"},"execution_count":null,"outputs":[]}]}